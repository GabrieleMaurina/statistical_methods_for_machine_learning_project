\section{Introduction}
\label{sec:intro}
This sections provides a very brief introduction to the topics discussed in the experiments.

Artificial neural networks (ANNs) are machine learning models loosely inspired by biological neural networks found in human and animal brains. ANNs consist of a set of nodes, i.e., neurons, connected to each other by edges, i.e., synapses. Nodes can send and receive signals, i.e. real numbers, through their edges. Edges typically have a weight that modulates the strength of the signal. Weights are adjusted during the training phase of a network. For each node, the sum of all weighted input signals is given to an activation function that can forward the signal to the output edges. This project uses Deep Neural Networks (DNNs) and their subclass Convolutional Neural Networks (CNNs).

\textbf{DNNs.} DNNs aggregate their nodes into layers. Usually nodes from the first layer forward signals to nodes in the second layer, which, in turn, forward signals to the next layer and the forwarding is repeated until the last layer is reached. In the case of multi-class classification, the last layer has a node for each label and the signal emitted represents the probability of that label. This project uses DNNs with non-linear activation functions and dense layers. A dense layer is a layer where each node is connected to all the nodes in the next layer. Such networks are also called Multi Layer Perceptrons (MLPs).

\textbf{CNNs.} CNNs, are a subclass of DNNs and are most commonly used in applications such as image recognition and computer vision. CNNs reduce the number of free parameters by only connecting neurons to other neurons withing a certain range. This allows the networks to be deeper, since fewer connections are necessary. Intuitively, in an image classification task, CNNs analyze the relation between adjacent pixels and discard the relation between pixels that are far away from each other in order to extrapolate the broader meaning of the image.
